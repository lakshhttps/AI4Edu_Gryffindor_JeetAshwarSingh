{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "badbda43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "33a63cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openpyxl in /nlsasfs/home/gpucbh/vyakti11/venv/lib/python3.10/site-packages (3.1.5)\n",
      "Requirement already satisfied: et-xmlfile in /nlsasfs/home/gpucbh/vyakti11/venv/lib/python3.10/site-packages (from openpyxl) (2.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b29604a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"../data/Train/labels_train.xlsx\")\n",
    "df.to_csv(\"labels.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b51ad694",
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_FOLDER = \"../data/Train\"  # Updated to correct path\n",
    "LABELS_FILE = \"labels.csv\"    \n",
    "IMG_SIZE = 224                \n",
    "SEQ_LEN = 16                  \n",
    "BATCH_SIZE = 4                \n",
    "EPOCHS = 10                   \n",
    "LR = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "03879355",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EngagementDataset(Dataset):\n",
    "    def __init__(self, df, video_dir, transform=None):\n",
    "        self.df = df\n",
    "        self.video_dir = video_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        vid_name = row['video']\n",
    "        \n",
    "        # 0 & 0.33 -> Class 0 (Low Engagement)\n",
    "        # 0.66 & 1 -> Class 1 (High Engagement)\n",
    "        label = 0.0 if float(row['label']) <= 0.33 else 1.0\n",
    "\n",
    "        vid_path = os.path.join(self.video_dir, vid_name)\n",
    "        if not os.path.exists(vid_path):\n",
    "             base = vid_name.split('.')[0]\n",
    "             for ext in ['.avi', '.wmv', '.webm', '.mp4']:\n",
    "                 if os.path.exists(os.path.join(self.video_dir, base + ext)):\n",
    "                     vid_path = os.path.join(self.video_dir, base + ext)\n",
    "                     break\n",
    "\n",
    "        frames = self._load_video(vid_path)\n",
    "\n",
    "        if self.transform:\n",
    "            frames = torch.stack([self.transform(f) for f in frames])\n",
    "        else:\n",
    "            to_tensor = transforms.ToTensor()\n",
    "            frames = torch.stack([to_tensor(f) for f in frames])\n",
    "            \n",
    "        return frames, torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "    def _load_video(self, path):\n",
    "        cap = cv2.VideoCapture(path)\n",
    "        frames = []\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        \n",
    "        if total_frames == 0:\n",
    "            # Return black frames if broken\n",
    "            return [np.zeros((IMG_SIZE, IMG_SIZE, 3), dtype=np.uint8)] * SEQ_LEN\n",
    "\n",
    "        # Pick 16 frames evenly\n",
    "        indices = np.linspace(0, total_frames-1, SEQ_LEN).astype(int)\n",
    "        \n",
    "        for i in range(total_frames):\n",
    "            ret, frame = cap.read()\n",
    "            if not ret: break\n",
    "            if i in indices:\n",
    "                frame = cv2.resize(frame, (IMG_SIZE, IMG_SIZE))\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frames.append(frame)\n",
    "                if len(frames) == SEQ_LEN: break\n",
    "        cap.release()\n",
    "\n",
    "        while len(frames) < SEQ_LEN:\n",
    "            frames.append(frames[-1] if frames else np.zeros((IMG_SIZE, IMG_SIZE, 3), dtype=np.uint8))\n",
    "            \n",
    "        return frames[:SEQ_LEN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46680abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNetLSTM, self).__init__()\n",
    "        resnet = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "\n",
    "        self.features = nn.Sequential(*list(resnet.children())[:-1])\n",
    "        \n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=512, hidden_size=128, batch_first=True)\n",
    "        \n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, c, h, w = x.size()\n",
    "        c_in = x.view(batch_size * seq_len, c, h, w)\n",
    "        \n",
    "\n",
    "        f_out = self.features(c_in)\n",
    "        f_out = f_out.view(batch_size, seq_len, 512)\n",
    "        \n",
    "\n",
    "        lstm_out, _ = self.lstm(f_out)\n",
    "        \n",
    "\n",
    "        final_state = lstm_out[:, -1, :] \n",
    "        return self.classifier(final_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611882d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training...\n",
      "Epoch 1 | Loss: 0.6788 | Val Acc: 53.33%\n",
      "Epoch 2 | Loss: 0.6358 | Val Acc: 66.67%\n",
      "Epoch 3 | Loss: 0.6109 | Val Acc: 66.67%\n",
      "Epoch 4 | Loss: 0.5578 | Val Acc: 66.67%\n",
      "Epoch 5 | Loss: 0.5534 | Val Acc: 66.67%\n",
      "Epoch 6 | Loss: 0.4912 | Val Acc: 66.67%\n",
      "Epoch 7 | Loss: 0.5810 | Val Acc: 66.67%\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(LABELS_FILE):\n",
    "    df = pd.read_csv(LABELS_FILE)\n",
    "    train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "    \n",
    "    tfms = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    train_ds = EngagementDataset(train_df, VIDEO_FOLDER, transform=tfms)\n",
    "    val_ds = EngagementDataset(val_df, VIDEO_FOLDER, transform=tfms)\n",
    "    \n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    model = ResNetLSTM().to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "    \n",
    "    print(\"Starting Training...\")\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for frames, labels in train_loader:\n",
    "            frames, labels = frames.to(device), labels.to(device).unsqueeze(1)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(frames)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for frames, labels in val_loader:\n",
    "                frames, labels = frames.to(device), labels.to(device).unsqueeze(1)\n",
    "                outputs = model(frames)\n",
    "                preds = torch.sigmoid(outputs) > 0.5\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "                \n",
    "        print(f\"Epoch {epoch+1} | Loss: {running_loss/len(train_loader):.4f} | Val Acc: {correct/total:.2%}\")\n",
    "\n",
    "    torch.save(model.state_dict(), \"binary_model.pth\")\n",
    "    print(\"Model Saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
